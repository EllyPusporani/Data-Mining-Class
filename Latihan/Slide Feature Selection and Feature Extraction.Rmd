---
title: "Feature Selection and Feature Extraction with R"
author: "Mawanda Almuhayar (06211750010019)"
date: "Elly Pusporani (06211750010023)"
output: slidy_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Feature Selection Methods :
1. Filter Method = Correlation Based Feature Selection (CFS)
2. Wrapper Method = Stepwise (Forward and Backward Elimination)
3. Embedded Method ***(Not Discussed)***


## Feature Extraction Methods :
1. Principal Components Analysis (PCA)
2. Kernel Principal Components Analysis (KPCA)


## Implementation with *Multinomial Logistic Regression* :
1. Using Original Data
2. Using Reduced Variable with Stepwise Method
3. Using Reduced Variable with CFS Method
4. Using Extracted Variable with PCA
5. Using Extracted Variable with Kernel PCA


## ***Auto*** Data Set
* *Auto dataset is the data about the gas mileage, horsepower, and other information for 392 vehicles.*
* Auto dataset consists of **392 observations on the following 9 variables.**
* This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. The dataset was used in the 1983 American Statistical Association Exposition.

## ***Auto*** Data Set Variables
* *mpg*  :   miles per gallon
* *cylinders*  :   Number of cylinders between 4 and 8
* *displacement*  :   Engine displacement (cu. inches)
* *horsepower*  :   Engine horsepower
* *weight*  :   Vehicle weight (lbs.)
* *acceleration*  :   Time to accelerate from 0 to 60 mph (sec.)
* *year*  :   Model year (modulo 100)
* *origin*  :   Origin of car (1. American, 2. European, 3. Japanese)
* *name*  :   Vehicle name


## Import the Data
### **Set the Workspace Location Folder and Data**
Workspace Location is the location where the files needed to run this syntax was saved, such as data files and other. This location can be changed as you wish.
```{r, echo=TRUE}
setwd("F:/S2/Semester 2/Data Mining")
data=read.csv("Auto.csv", header=T, na.strings ="?")
```



## Overview Data
### **Display the Structure of the Data**
```{r, echo=TRUE}
str(data)
```
## Overview Data *(Continued)*
### **View Some of the First Sequence of The Data**
```{r, echo=TRUE}
head(data)
```
## Overview Data *(Continued)*
### **Summary of the Data**
```{r, echo=TRUE}
summary(data)
```
## Overview Data *(Continued)*
### **Attaching Variables**
```{r, echo=TRUE}
attach(data)
names(data)
```



## Data Cleaning
**Imputation Based on Modeling**
```{r, echo=TRUE}
dim(data)
data[!complete.cases(data),]
```

## Data Cleaning *(Continued)*
**Imputation Based on Modeling *(Continued)***
```{r, echo=TRUE}
missingdata = is.na(data$horsepower)
sum(missingdata)
model = lm(horsepower ~ mpg+displacement+weight+acceleration, data=data)
data$horsepower[missingdata] = predict(model, newdata = data[missingdata,])
data[!complete.cases(data),]
```

## Data Cleaning *(Continued)*
**Imputation Based on Modeling *(Continued)***
```{r, echo=TRUE}
databaru = data[,-c(2,7,9)]
databaru[,ncol(databaru)] = as.factor(databaru[,ncol(databaru)])
data_x = databaru[,-ncol(databaru)]; head(data_x, 10)
```


## Checking Correlation Accross Independent Variables
Correlation  can be checked using correlation matrix from all independen variables.
```{r, echo=TRUE, results="hide"}
library(corrplot)
```
```{r, echo=TRUE}
cor = cor(data_x); cor
```

## Checking Correlation Accross Independent Variables *(Continued)*
Checking correlation visually using matrix scatterplot from all independent variables.
```{r, echo=TRUE}
pairs(data_x, col="red")
```

## Checking Correlation Accross Independent Variables *(Continued)*
Checking correlation visually using correlation plot from all independent variables.
```{r pressure, echo=TRUE}
corrplot(cor, method="ellipse")
```




## Feature Selection
### Using Correlation Based Feature Selection (CFS)
Using Library Biocomb, variables reduced using filter method using CFS.
```{r, echo=TRUE, results="hide"}
library(Biocomb)
```
Variables produced by CFS method.
```{r, echo=TRUE}
select.cfs(databaru)
data_cfs = databaru[,c(select.cfs(databaru)$Index, ncol(databaru))]
head(data_cfs, 10)
```



## Feature Extraction
### Principal Components Analysis (PCA)
```{r, echo=TRUE}
pca_data = prcomp(~., data=data_x, scale = TRUE)
summary(pca_data)
```

```{r, echo=TRUE, include=FALSE}
names(pca_data)
names(summary(pca_data))
```

## Feature Extraction *(Continued)*
### Principal Components Analysis (PCA) *(Continued)*
Summary and number of components produced by PCA.
```{r, echo=TRUE}
Eigenvalue   = eigen(cor)$values
pca_komponen = summary(pca_data)$importance
pca_hasil    = rbind(Eigenvalue, pca_komponen); pca_hasil
pca_coef     = -pca_data$rotation;  pca_coef
```


## Feature Extraction *(Continued)*
### Principal Components Analysis (PCA) *(Continued)*
Scree plot from components extracted from PCA, and total cumulative proportion variance explained by the components.
```{r, echo=TRUE}
par(mfrow=c(1,2))
plot(Eigenvalue, xlab = "Principal Component", ylab = "Eigenvalue", type='b', main="Scree Plot"); abline(h=0.7, lty=2, col="red")
plot(pca_hasil[4,], xlab = "Principal Component", ylab = "Cumulative Proportion of Variance Explained", ylim = c(0,1), type='b')
```

## Feature Extraction *(Continued)*
### Principal Components Analysis (PCA) *(Continued)*
PCA scores which is PC1 and PC2 using as new independent variables for next analysis.
```{r, echo=TRUE}
pca_score = -pca_data$x; head(pca_score, 10)
```

## Feature Extraction *(Continued)*
### Principal Components Analysis (PCA) *(Continued)*
Scatterplot PC1 and PC2 that extracted from PCA.
```{r, echo=TRUE}
plot(pca_score[,1], pca_score[,2], col=as.integer(databaru$origin), xlab="PC1", ylab="PC2", pch=16)
```

## Feature Extraction *(Continued)*
### Principal Components Analysis (PCA) *(Continued)*
New data frame after extracted variables using PCA.
```{r, echo=TRUE}
data_x_pca = pca_score[,c(1,2)]
data_pca   = cbind(data_x_pca, databaru[,ncol(databaru)]);
colnames(data_pca)[3] = "origin"
data_pca   = as.data.frame(data_pca)
head(data_pca, 10)
```




## Feature Extraction *(Continued)*
### Kernel Principal Components Analysis (Kernel PCA)
***Using Linear Kernel Function***
Variable extraction by Kernel PCA using Library kernlab. Here the Summary that produced by Kernel PCA (KPCA).
```{r, echo=TRUE}
library(kernlab)
kpca_data <- kpca(~., data=data_x, kernel="vanilladot", kpar=list(), features=2)
kpca_pcv = pcv(kpca_data)
eig(kpca_data)
kpca_score = rotated(kpca_data)
```

## Feature Extraction *(Continued)*
### Kernel Principal Components Analysis (Kernel PCA) *(Continued)*
Scatterplot KPC1 and KPC2 that extracted from KPCA.
```{r, echo=TRUE}
plot(kpca_score, col=as.integer(databaru$origin), xlab="PC1", ylab="PC2", pch=16)
```

## Feature Extraction *(Continued)*
### Kernel Principal Components Analysis (Kernel PCA) *(Continued)*
New data frame after extracted variables using PCA.
```{r, echo=TRUE}
data_x_kpca = kpca_score[,c(1,2)]
data_kpca   = cbind(data_x_kpca, databaru[,ncol(databaru)]); colnames(data_kpca) = c("KPCA1", "KPCA2", "origin")
data_kpca   = as.data.frame(data_kpca)
head(data_kpca, 10)
```




## Implementation using Multinomial Logistic Regression
### 1. Using Original Data
***Partisi Training 80%, Testing 20%***
```{r, echo=TRUE}
library(nnet)
head(databaru, 10)
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 1. Using Original Data *(Continued)*
***Partisi Training 80%, Testing 20% *(Continued)**
```{r, echo=TRUE}
n       = nrow(databaru); n
n.train = floor(n*0.8);   n.train
n.test  = n-n.train;     n.test
target  = as.factor(databaru$origin)
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 1. Using Original Data *(Continued)*
***Partisi Training 80%, Testing 20% *(Continued)**
```{r, echo=TRUE}
set.seed(100)
train.index  = sample(seq(n), size=n.train)
data.train1  = databaru[train.index,];  head(data.train1, 10); dim(data.train1)
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 1. Using Original Data *(Continued)*
***Partisi Training 80%, Testing 20% *(Continued)**
```{r, echo=TRUE}
data.test1   = databaru[-train.index,]; head(data.test1, 10);  dim(data.test1)

target.train1 = target[train.index]
target.test1  = target[-train.index]
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 1. Using Original Data *(Continued)*
```{r, echo=TRUE}
rlogmult1 = multinom(target~., data=databaru[1:5], subset=train.index)
summary(rlogmult1)
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 1. Using Original Data *(Continued)*
***Classification Table for Training Original Data***
```{r, echo=TRUE}
pred.train1     = predict(rlogmult1)
tbl.clas.train1 = table(pred.train1, target.train1); tbl.clas.train1
akurasi.train1  = mean(pred.train1==target.train1)
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 1. Using Original Data *(Continued)*
***Classification Table for Testing Original Data***
```{r, echo=TRUE}
pred.test1      = predict(rlogmult1, newdata=data.test1)
tbl.clas.test1  = table(pred.test1, target.test1); tbl.clas.test1
akurasi.test1   = mean(pred.test1==target.test1)
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 1. Using Original Data *(Continued)*
***Accuration Rate for Original Data***
```{r, echo=TRUE}
akurasi.ori = rbind(akurasi.train1, akurasi.test1)
akurasi.ori
```



## Implementation using Multinomial Logistic Regression *(Continued)*
### 2. Using reduced Data with Stepwise Method
```{r, echo=TRUE}
step(rlogmult1)
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 2. Using reduced Data with Stepwise Method *(Continued)*
```{r, echo=TRUE}
data_stepwise = databaru[-5]
head(data_stepwise, 10)
```

```{r, echo=TRUE, include=FALSE}
n
n.train
n.test

set.seed(100)
train.index = sample(seq(n), size=n.train)
data.train2  = data_stepwise[train.index,];  head(data.train2, 10); dim(data.train2)
data.test2   = data_stepwise[-train.index,]; head(data.test2, 10);  dim(data.test2)

target.train2 = target[train.index]
target.test2  = target[-train.index]
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 2. Using reduced Data with Stepwise Method *(Continued)*
```{r, echo=TRUE}
rlogmult2 = multinom(target~., data=data_stepwise[1:4], subset=train.index)
summary(rlogmult2)
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 2. Using reduced Data with Stepwise Method *(Continued)*
***Classification Table for Training Data Stepwise***
```{r, echo=TRUE}
pred.train2     = predict(rlogmult2)
tbl.clas.train2 = table(pred.train2, target.train2); tbl.clas.train2
akurasi.train2  = mean(pred.train2==target.train2)
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 2. Using reduced Data with Stepwise Method *(Continued)*
***Classification Table for Testing Data Stepwise***
```{r, echo=TRUE}
pred.test2      = predict(rlogmult2, newdata=data.test2)
tbl.clas.test2  = table(pred.test2, target.test2); tbl.clas.test2
akurasi.test2   = mean(pred.test2==target.test2)
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 2. Using reduced Data with Stepwise Method *(Continued)*
***Accuration Rate for Data Stepwise***
```{r, echo=TRUE}
akurasi.stepwise = rbind(akurasi.train2, akurasi.test2)
akurasi.stepwise
```




## Implementation using Multinomial Logistic Regression *(Continued)*
### 3. Using reduced Data with CFS Method
```{r, echo=TRUE}
head(data_cfs, 10)
```

```{r, echo=TRUE, include=FALSE}
n
n.train
n.test

set.seed(100)
train.index = sample(seq(n), size=n.train)
data.train3  = data_cfs[train.index,];  head(data.train3, 10); dim(data.train3)
data.test3   = data_cfs[-train.index,]; head(data.test3, 10);  dim(data.test3)

target.train3 = target[train.index]
target.test3  = target[-train.index]
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 3. Using reduced Data with CFS Method *(Continued)*
```{r, echo=TRUE}
rlogmult3 = multinom(target~., data=data_cfs[1], subset=train.index)
summary(rlogmult3)
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 3. Using reduced Data with CFS Method *(Continued)*
***Classification Table for Training Data CFS***
```{r, echo=TRUE}
pred.train3     = predict(rlogmult3)
tbl.clas.train3 = table(pred.train3, target.train3); tbl.clas.train3
akurasi.train3  = mean(pred.train3==target.train3)
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 3. Using reduced Data with CFS Method *(Continued)*
***Classification Table for Testing Data CFS***
```{r, echo=TRUE}
pred.test3      = predict(rlogmult3, newdata=data.test3)
tbl.clas.test3  = table(pred.test3, target.test3); tbl.clas.test3
akurasi.test3   = mean(pred.test3==target.test3)
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 3. Using reduced Data with CFS Method *(Continued)*
***Accuration Rate Data CFS***
```{r, echo=TRUE}
akurasi.cfs = rbind(akurasi.train3, akurasi.test3)
akurasi.cfs
```



## Implementation using Multinomial Logistic Regression *(Continued)*
### 4. Using Extracted Data with PCA Method
```{r, echo=TRUE}
head(data_pca, 10)
```

```{r, echo=TRUE, include=FALSE}
n
n.train
n.test

set.seed(100)
train.index = sample(seq(n), size=n.train)
data.train4  = data_pca[train.index,];  head(data.train4, 10); dim(data.train4)
data.test4   = data_pca[-train.index,]; head(data.test4, 10);  dim(data.test4)

target.train4 = target[train.index]
target.test4  = target[-train.index]
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 4. Using Extracted Data with PCA Method *(Continued)*
```{r, echo=TRUE}
rlogmult4 = multinom(target~., data=data_pca[1:2], subset=train.index)
summary(rlogmult4)
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 4. Using reduced Data with PCA Method *(Continued)*
***Classification Table for Training Data PCA***
```{r, echo=TRUE}
pred.train4     = predict(rlogmult4)
tbl.clas.train4 = table(pred.train3, target.train4); tbl.clas.train4
akurasi.train4  = mean(pred.train4==target.train4)
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 4. Using reduced Data with PCA Method *(Continued)*
***Classification Table for Testing Data PCA***
```{r, echo=TRUE}
pred.test4      = predict(rlogmult4, newdata=data.test4)
tbl.clas.test4  = table(pred.test4, target.test4); tbl.clas.test4
akurasi.test4   = mean(pred.test4==target.test4)
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 4. Using reduced Data with PCA Method *(Continued)*
***Accuration Rate Data PCA***
```{r, echo=TRUE}
akurasi.pca = rbind(akurasi.train4, akurasi.test4)
akurasi.pca
```




## Implementation using Multinomial Logistic Regression *(Continued)*
### 5. Using Extracted Data with KPCA Method
```{r, echo=TRUE}
head(data_kpca, 10)
```

```{r, echo=TRUE, include=FALSE}
n
n.train
n.test

set.seed(100)
train.index = sample(seq(n), size=n.train)
data.train5  = data_kpca[train.index,];  head(data.train5, 10); dim(data.train5)
data.test5   = data_kpca[-train.index,]; head(data.test5, 10);  dim(data.test5)

target.train5 = target[train.index]
target.test5  = target[-train.index]
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 5. Using Extracted Data with KPCA Method *(Continued)*
```{r, echo=TRUE}
rlogmult5 = multinom(target~., data=data_kpca[1:2], subset=train.index)
summary(rlogmult5)
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 5. Using reduced Data with KPCA Method *(Continued)*
***Classification Table for Training Data KPCA***
```{r, echo=TRUE}
pred.train5     = predict(rlogmult5)
tbl.clas.train5 = table(pred.train5, target.train5); tbl.clas.train5
akurasi.train5  = mean(pred.train5==target.train5)
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 5. Using reduced Data with KPCA Method *(Continued)*
***Classification Table for Testing Data KPCA***
```{r, echo=TRUE}
pred.test5      = predict(rlogmult5, newdata=data.test5)
tbl.clas.test5  = table(pred.test5, target.test5); tbl.clas.test5
akurasi.test5   = mean(pred.test5==target.test5)
```

## Implementation using Multinomial Logistic Regression *(Continued)*
### 5. Using reduced Data with PCA Method *(Continued)*
***Accuration Rate Data KPCA***
```{r, echo=TRUE}
akurasi.kpca = rbind(akurasi.train5, akurasi.test5)
akurasi.kpca
```



## Total Accuration and Evaluation Method
```{r, echo=TRUE}
akurasi.all = cbind(akurasi.ori, akurasi.stepwise, akurasi.cfs, akurasi.pca, akurasi.kpca)
rownames(akurasi.all) = c("Akurasi Training", "Akurasi Testing")
colnames(akurasi.all) = c("Data Original", "Data Stepwise", "Data CFS", "Data PCA", "Data KPCA")
akurasi.all
```



## Conclusion
1. In training data, the highest accuracy was obtained when using data that has been processed with Feature Selection using Stepwise Method
2. In the data testing, the highest accuracy can be obtained when using the original data
3. The best data selected is the original data
4. In this case Feature Selection and Feature Extraction does not increased the accuracy level significantly

##Thank You